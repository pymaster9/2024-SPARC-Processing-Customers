For this problem, I will assume that “efficiently” refers to maximizing the number of customers processed per minute and “fairly” refers to minimizing the difference between the small and large boxes' average wait time and minimizing the standard deviation of the time. I also assume that the cashier does not start with any customers, and they have to wait until their first customer to start processing customers. Finally, I will assume that the cashier knows the size of the boxes of each customer and the time they have been waiting.

Based on this information, I created a Monte-Carlo simulation that would run a simulated version of this problem for 100 hours, allowing for the cashier to use different strategies. It is important to note that all simulations use the same random factors and the same random seed (31415926) to ensure that one strategy is not simply getting more favorable luck than another. I then created a function to find and display all relevant statistics: average time taken for customers with small and large boxes, standard deviations from the average time, and the number of customers processed per minute.

Now I could start testing different strategies! I first started with a “first come first serve” strategy that chooses the customer that has been in the queue for the longest. Since this was only meant to be a baseline strategy, it would perform significantly worse than other strategies to come, only processing 0.998 customers per minute and having a standard deviation in waiting times of over 3,000 seconds. However, it did not differentiate between customers with small and large baskets, with a difference of average wait time of only 62 seconds!

I then tried an “alternating baskets” strategy, which cycles between choosing the earliest customer with a small box and then the earliest customer with a large box. This did not perform as well as I had hoped, only processing 0.998 customers per minute, having a standard deviation in times of almost 3,380 seconds, and making people with smaller baskets almost 15 times longer than people with larger baskets. However, I believed that there could be potential in upgrading my strategy to a “periodic alteration” strategy that would cycle between a period of choosing the earliest customer with a small box and then the earliest customer with a large box. Unfortunately, it did worse than its predecessor even while maximizing and minimizing respective fields to arrive at a period length of 2 (1 would make it essentially the same algorithm), 

I then tried a “minimizing queues” strategy, which picks from the pool of customers with the same sized basket that has a greater size, thus minimizing that queue. This strategy performed fairly well, having average fairness, and being vastly more efficient than I had designed before. From here, I thought of the fairest strategy possible, abandoning efficiency, and came up with a fully random strategy. Surprisingly, this strategy was fairly efficient, processing 0.998 customers per minute. Therefore, I decided to pursue the concept of random strategies, creating a “randomized by ratio” strategy, which sets the percentage chance of selecting someone with a small basket over someone with a larger one (0.5 is perfectly even). After optimizing the ratio, though it had a lower standard deviation of customer waiting times (1187 seconds vs 2699 seconds), the difference in customers with small baskets’ and large baskets’ waiting times was higher (2592 seconds vs 7 seconds). However, it was vastly more efficient, being able to process 1 customer per minute versus 0.998 customers!

I then decided to compare these strategies by running them with their respective optimized parameters by calculating the percentile of each strategy over all 3 strategies and then averaging them. After doing this, I realized that my “minimizing queues” strategy performed the best by a large margin, followed by my “randomized by ratio” strategy narrowly beating its fully random cousin. However, I realized that this method of comparing strategies overprioritized my two fairness parameters, as they make up 2/3 of each strategy’s ranking, while 1/3 of each strategy’s ranking is its efficiency. I then decided to compensate for this by using a weighted average to ensure that the efficiency of each strategy is treated equally to its fairness. Upon doing this, there was a tie for first place between my “randomized by ratio” and “minimizing queues” strategies, leading to a moral dilemma. After pondering the question of which strategy should win, I realized that since the “minimizing queues” strategy has customers approximately knowing when they would be selected, while my “randomized by ratio” strategy was inherently random, I decided that my “minimizing queues” strategy was the fairer one, and therefore the winner!

Therefore, the fairest and most efficient strategy for our cashier is the “minimizing queues” strategy, which is as follows:
- The cashier groups the customers with small and large baskets into two groups and decides which one is bigger
- Process the customer who has been in the larger group for the longest time to “minimize that queue”

Based on my simulations, this strategy was able to process 0.999 customers per minute with a difference of 2023 seconds between people with larger and smaller baskets and has customers’ waiting times having an average difference of 1073 seconds from their expected waiting times.
